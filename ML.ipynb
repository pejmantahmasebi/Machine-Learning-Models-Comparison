{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc38d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                             AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier,\n",
    "                             VotingClassifier, StackingClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def compare_ml_models(X, y, test_size=0.2, random_state=42, cv=5):\n",
    "    \"\"\"\n",
    "    Compare multiple machine learning models simultaneously and display results\n",
    "    \n",
    "    Parameters:\n",
    "    X: Input features\n",
    "    y: Target labels\n",
    "    test_size: Test data size ratio\n",
    "    random_state: Random seed for reproducibility\n",
    "    cv: Number of cross-validation folds\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame containing results of all models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        # Linear Models\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=random_state),\n",
    "        'Ridge Classifier': RidgeClassifier(random_state=random_state),\n",
    "        'SGD Classifier': SGDClassifier(random_state=random_state),\n",
    "        \n",
    "        # SVM Models\n",
    "        'Linear SVM': LinearSVC(random_state=random_state),\n",
    "        'RBF SVM': SVC(kernel='rbf', probability=True, random_state=random_state),\n",
    "        'Polynomial SVM': SVC(kernel='poly', probability=True, random_state=random_state),\n",
    "        \n",
    "        # Tree-based Models\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
    "        'Random Forest': RandomForestClassifier(random_state=random_state),\n",
    "        'Extra Trees': ExtraTreesClassifier(random_state=random_state),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=random_state),\n",
    "        'Hist Gradient Boosting': HistGradientBoostingClassifier(random_state=random_state),\n",
    "        \n",
    "        # Boosting Algorithms\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=random_state),\n",
    "        'XGBoost': XGBClassifier(random_state=random_state, verbosity=0),\n",
    "        'LightGBM': LGBMClassifier(random_state=random_state, verbose=-1),\n",
    "        'CatBoost': CatBoostClassifier(random_state=random_state, verbose=0),\n",
    "        \n",
    "        # Bayesian Models\n",
    "        'Gaussian NB': GaussianNB(),\n",
    "        'Bernoulli NB': BernoulliNB(),\n",
    "        \n",
    "        # Nearest Neighbors\n",
    "        'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "        'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "        'KNN (k=7)': KNeighborsClassifier(n_neighbors=7),\n",
    "        \n",
    "        # Neural Networks\n",
    "        'MLP (1 layer)': MLPClassifier(hidden_layer_sizes=(100,), random_state=random_state, max_iter=1000),\n",
    "        'MLP (2 layers)': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=random_state, max_iter=1000),\n",
    "        \n",
    "        # Discriminant Analysis\n",
    "        'LDA': LinearDiscriminantAnalysis(),\n",
    "        'QDA': QuadraticDiscriminantAnalysis(),\n",
    "        \n",
    "        # Ensemble Methods\n",
    "        'Bagging': BaggingClassifier(random_state=random_state),\n",
    "        \n",
    "        # Gaussian Process\n",
    "        'Gaussian Process': GaussianProcessClassifier(random_state=random_state),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    def train_and_evaluate(model_name, model):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate evaluation metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            # Calculate ROC AUC if possible\n",
    "            try:\n",
    "                if hasattr(model, \"predict_proba\"):\n",
    "                    y_proba = model.predict_proba(X_test)\n",
    "                    if y_proba.shape[1] > 2:  # Multi-class\n",
    "                        roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
    "                    else:\n",
    "                        roc_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "                else:\n",
    "                    roc_auc = np.nan\n",
    "            except:\n",
    "                roc_auc = np.nan\n",
    "            \n",
    "            # Calculate cross-validation score\n",
    "            cv_scores = cross_val_score(model, X, y, cv=cv, n_jobs=1)\n",
    "            cv_mean = np.mean(cv_scores)\n",
    "            cv_std = np.std(cv_scores)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                'Model': model_name,\n",
    "                'Accuracy': accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1 Score': f1,\n",
    "                'ROC AUC': roc_auc,\n",
    "                'CV Mean Score': cv_mean,\n",
    "                'CV Std': cv_std,\n",
    "                'Training Time (s)': training_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {model_name}: {str(e)}\")\n",
    "            return {\n",
    "                'Model': model_name,\n",
    "                'Accuracy': np.nan,\n",
    "                'Precision': np.nan,\n",
    "                'Recall': np.nan,\n",
    "                'F1 Score': np.nan,\n",
    "                'ROC AUC': np.nan,\n",
    "                'CV Mean Score': np.nan,\n",
    "                'CV Std': np.nan,\n",
    "                'Training Time (s)': np.nan\n",
    "            }\n",
    "    \n",
    "    # Execute models in parallel\n",
    "    with ThreadPoolExecutor(max_workers=min(len(models), 8)) as executor:\n",
    "        futures = [executor.submit(train_and_evaluate, name, model) for name, model in models.items()]\n",
    "        results = [future.result() for future in futures]\n",
    "    \n",
    "    # Create results table\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by='F1 Score', ascending=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "results = compare_ml_models(X, y)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
